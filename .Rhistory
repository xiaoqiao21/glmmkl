list(Score = cv$dt[, max(test.auc.mean)],
Pred = cv$pred)
}
xgb_cv_bayes(2, 3, .6)
library(h2o)
h2o.init()
iris.hex <- as.h2o(iris)
iris.dl <- h2o.deeplearning(x = 1:4, y = 5, nfolds = 5, keep_cross_validation_predictions = T, training_frame = iris.hex, seed=123456, hidden = c(200, 200))
predictions <- h2o.predict(iris.dl, iris.hex)
h2o.varimp(iris.dl)
lapply(1:100, function(x) 10 + sample(50, sample(4), replace = T))
hidden_opt = lapply(1:100, function(x) 10 + sample(50, sample(4), replace = T))
l1_opt = seq(1e-6,1e-3,1e-6)
seq(1e-6,1e-3,1e-6)
l1_opt = exp(log(seq(1e-6,1e-3,1e-6)))
l1_opt
l1_opt = exp(seq(log(1e-6), log(1e-3),1e-6))
l1_opt
l1_op1[2]/l1_opt[1]
l1_opt[2]/l1_opt[1]
l1_opt[3]/l1_opt[2]
l1_opt[1000]/l1_opt[100]
l1_opt[100]/l1_opt[100]
l1_opt[101]/l1_opt[100]
l1_opt[110]/l1_opt[100]
l1_opt[11]/l1_opt[1]
?h2o.deeplearning
lapply(1:100, function(x) 10 + sample(50, sample(4), replace = T))
lapply(1:100, function(x) 10 + sample(400, sample(4), replace = T))
sample(4)
?sample
setwd("F:/kaggle/housing price/data")
setwd("~/")
library(h2o)
h2o.xgboost.available()
h2o.init()
h2o.xgboost.available()
install.packages('h2o')
library(h2o)
h2o.init()
920+84+50+2497.22+103.24-1196.99-103.24+627.54+27.75+191.98
install.packages('lime')
library(lime)
88*4*9*.22
749/4225
160+2500*.02+1250*.04
120*800000
262330+1049945+211311+3913726+60000000+2998098+68952+1499360+3958452+4003683
4003683*.25+3958452*.1+1499360*.1+2998098*.3+60000000*.1+3913726*.3+1049945*.02
library(dlstats)
aa=cran_stats('JumpTest')
sum(aa$downloads)
200+500+900+2500
200+500+1500
106,780.23+11,225.83+4062.48+501.56+43.05+3132.64+1111.89
106780.23+11225.83+4062.48+501.56+43.05+3132.64+1111.89
7600*12
3854*27
library(xgboost)
install.packages('xgboost')
library(xgboost)
data(agaricus.train, package = "xgboost")
dtrain <- xgb.DMatrix(agaricus.train$data,
label = agaricus.train$label)
cv_folds <- KFold(agaricus.train$label, nfolds = 5,
stratified = TRUE, seed = 0)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$dt[, max(test.auc.mean)],
Pred = cv$pred)
}
library(rBayesianOptimization)
install.packages('rBayesianOptimization')
library(rBayesianOptimization)
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
cv_folds <- KFold(agaricus.train$label, nfolds = 5,
stratified = TRUE, seed = 0)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$dt[, max(test.auc.mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
?xgb.cv
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = list("rmse","auc"),
max_depth = 3, eta = 1, objective = "binary:logistic")
cv
cv$dt
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = 'auc',
max_depth = 3, eta = 1, objective = "binary:logistic")
cv
cv$niter
cv$folds
cv$evaluation_log
cv$evaluation_log[,max(test_auc_mean)]
cv$pred
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = 'auc',
max_depth = 3, eta = 1, objective = "binary:logistic", prediction = T)
cv$pred
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0,
prediction = T)
list(Score = cv$evaluation_log[,max(test_auc_mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$evaluation_log[,max(test_auc_mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
library(h2o)
h2o.init()
agaricus.train
train <- cbind(agaricus.train$label, agaricus.train$data)
head(train)
dim(train)
train[1:5,]
agaricus.train$label
train <- data.frame(label = agaricus.train$label, data = agaricus.train$data)
train <- data.frame(label = agaricus.train$label, data = as.data.frame(agaricus.train$data))
agaricus.train$data
as.matrix(agaricus.train$data)
train <- data.frame(label = agaricus.train$label, data = as.data.frame(as.matrix(agaricus.train$data)))
train
train <- as.h2o(train)
train
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
?h2o.randomForest
irisPath = system.file("extdata", "iris.csv", package="h2o")
iris.hex = h2o.uploadFile(localH2O, path = irisPath, key = "iris.hex")
data(iris)
iris
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfhex
rfHex
train <- data.frame(label = as.factor(agaricus.train$label), data = as.data.frame(as.matrix(agaricus.train$data)))
train <- as.h2o(train)
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
h2o.performance(rfHex)
rfHex$mode
rfHex@model
h2o.performance(rfHex)$cross_validation_metrics
h2o.performance(rfHex)@cross_validation_metrics
h2o.auc(rfHex)
rfHex
slot(rfHex@model)
unslot(rfHex@model)
rfHex
data(iris)
library(data.table)
setDT(iris)
iris
iris_tr <- iris[, Species %in% c('setosa', 'virginica')]
iris_tr
iris_tr <- iris[Species %in% c('setosa', 'virginica')]
iris_tr
train <- as.h2o(iris_tr)
train
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex <- h2o.randomForest(y = 'Species',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
rfHex <- h2o.randomForest(y = 'Species',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
prostate_path <- system.file("extdata", "prostate.csv", package = "h2o")
prostate <- h2o.uploadFile(prostate_path)
prostate
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
prostate[,2] <- as.factor(prostate[,2])
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
perf <- h2o.performance(model, prostate)
h2o.auc(perf)
prostate
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = prostate)
rfHex
h2o.auc(rfHex)
h2o.performance(rfHex)
?h2o.performance
h2o.performance(rfHex, train = T)
h2o.performance(rfHex, valid = T)
h2o.performance(rfHex)
h2o.auc(h2o.performance(rfHex))
h2o.auc(rfHex)
h2o.predict(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
h2o.cross_validation_models(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
h2o.cross_validation_redictions(rfHex)
h2o.cross_validation_predictions(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
?h2o.cross_validation_holdout_predictions
h2o.cross_validation_holdout_predictions(rfHex)[,p1]
h2o.cross_validation_holdout_predictions(rfHex)$p1
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
seed = 42,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = prostate)
h2o.auc(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)$p1
?h2o.randomForest
rf_cv_prediction <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree, nbins_cats) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntress = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
nbins_cats = nbins_cats,
keep_cross_validation_predictions = T,
keep_cross_validation_predictions = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
rf_cv_bayes <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree, nbins_cats) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntress = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
nbins_cats = nbins_cats,
keep_cross_validation_predictions = T,
keep_cross_validation_predictions = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
learn_rate = c(.001, .01),
sample_rate = c(.1, 1),
col_sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
learn_rate = c(.001, .01),
sample_rate = c(.1, 1),
col_sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, sample_rate, col_sample_rate_per_tree, col_sample_rate_change_per_level, mtries) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
sample_rate = sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
col_sample_rate_change_per_level = col_sample_rate_change_per_level,
mtries = mtries,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
mtries = c(10L, 20L)
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
mtries = c(10L, 20L),
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, sample_rate, col_sample_rate_per_tree, col_sample_rate_change_per_level) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
sample_rate = sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
col_sample_rate_change_per_level = col_sample_rate_change_per_level,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
library(h2o)
?h2o.deeplearning
?h2o.randomForest
29.78-2.99
install.packages('ROI')
library(ROC)
library(ROI)
150/4
6.5*3+16
6.5*3+48/3
2^12-1
install.packages('RcppArmadillo')
install.packages('installr')
library(installr)
updateR(copy_packages = F, keep_old_packages = F)
?system
install.packages('RobustAIC')
install.packages('devtools')
library(BiocManager)
install()
install('devtools')
library(devtools)
setwd("F:/Rpack/RobustAUC")
use_rcpp_armadillo()
use_package('pROC', type = 'depends')
use_package('stats')
use_mit_license(name = 'Pei Fen Kuan')
use_gpl3_license('RobustAUC')
use_gpl3_license('RobustAUC')
setwd("F:/Rpack")
create_package('glmmkl')
setwd("F:/Rpack/RobustAUC")
use_gpl3_license('RobustAUC')
load_all()
load_all()
document()
build()
setwd("F:/Rpack/RobustAUC")
load_all()
document()
build()
file.edit('DESCRIPTION')
load_all()
document()
build()
check(manual = T)
setwd("F:/Rpack/glmmkl")
use_mit_license(name = 'Kaiqiao Li')
getwd()
setwd("F:/Rpack/glmmkl")
use_mit_license(name = 'Kaiqiao Li')
load_all()
document()
setwd("F:/Rpack/RobustAUC")
use_gpl3_license('RobustAUC')
load_all()
document()
build()
setwd("F:/Rpack/glmmkl")
use_mit_license(name = 'Kaiqiao Li')
use_gpl3_license('RobustAUC')
